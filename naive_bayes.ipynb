{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRC0e0KhQ0S",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "Naive Bayes is one of the most popular classifcation algorithims and one that runs on the basis of the bayes equation of probability. It is highly efficent for problems with lots of numeral probabilities involved. Which makes it perfect for our dataset of  acute inflammations of the urinary bladder and acute\n",
        "nephritises. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd1UlMnhT2s",
        "colab_type": "text"
      },
      "source": [
        "### Importing the libraries\n",
        "These are the three go to libraries for most ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGPUQaHhXfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 507,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1VMqkGvhc3-",
        "colab_type": "text"
      },
      "source": [
        "### Importing the dataset\n",
        "I imported the dataset through Pandas dataframe then using iloc assign everything besides the last column as our independent variable(s) or X and the last column as our dependent variable or Y. The name of the dataset has to be updated and it must be in the same folder as your .py file or uploaded on Jupyter Notebooks or Google Collab.\n",
        "\n",
        "For this specific dataset, we can adjust the y dataframe reading to diagnose one disease ('6: ' or '7: ') or both ('6: '). The while loop on the bottom is only activated to categorize the possibilites of both diseases togehter when we choose to predict them together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oa0ydlCkqP2M",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('diagnosis2.csv')\n",
        "\n",
        "X = dataset.iloc[:, :6].values\n",
        "y = dataset.iloc[:, 6:].values\n",
        "\n",
        "if len(y[0]) == 2:\n",
        "  var = 0\n",
        "  row = 0\n",
        "  col1 = 0\n",
        "  col2 = 1\n",
        "  newcol = []\n",
        "  \n",
        "  while var < len(y):\n",
        "      if y[row, col1] == 0 and y[row, col2] == 0:\n",
        "          newcol.append(['A'])\n",
        "          row = row + 1\n",
        "          var = var + 1\n",
        "      elif y[row, col1] == 1 and y[row, col2] == 0:\n",
        "          newcol.append(['B'])\n",
        "          row = row + 1\n",
        "          var = var + 1\n",
        "      elif y[row, col1] == 0 and y[row, col2] == 1:\n",
        "          newcol.append(['C'])\n",
        "          row = row + 1\n",
        "          var = var + 1\n",
        "      elif y[row, col1] == 1 and y[row, col2] == 1:\n",
        "          newcol.append(['D'])\n",
        "          row = row + 1\n",
        "          var = var + 1\n",
        "\n",
        "  y = np.array(newcol)"
      ],
      "execution_count": 509,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxIPVyMhmKp",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set\n",
        "Here I used a normal 80/20 test size and  assigned 'random_state' None.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVzJWAXIhxoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = None)"
      ],
      "execution_count": 510,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW3c7UYih0hT",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling\n",
        "We can feature scale our data to make it easier for our model to train on our data and give us accurate results that aren't shifted by the presence of extreme outliers. It's not necessary, but helpful for getting more accurate results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fQlDPKCh8sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 511,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6jCOCQiAmP",
        "colab_type": "text"
      },
      "source": [
        "### Training the Naive Bayes model on the Training set\n",
        "Simple training the model using a Gaussian NB function which is the most common one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0pFVAmciHQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYVQH-l5NpE",
        "colab_type": "text"
      },
      "source": [
        "### Predicting the Test set results\n",
        "This uses the X_test to predict our diagnosis  and then compares the prediction and actual result in a concatenated print."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6VMTb2O4hwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Hwj34ziWQW",
        "colab_type": "text"
      },
      "source": [
        "### Making the Confusion Matrix\n",
        "The confusion matrix is a useful metric for classification models to allow us to visualize the correct positive, false positive, false negative, and correct negative predictions as well as a ultimate accuracy score on the bottom. \n",
        "\n",
        "Our model performs to nearly 90% and 95% accuracy for diagnosing the diseases exclusivly and almost 100% accuracy for diagnosing them togehter.\n",
        "Normal physician diagnosing accuracy is around 80%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6bpZwUiiXic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}